{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oikn2018/CS6910_assignment_3/blob/main/sweep_w_att.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KreNpqQWnde",
        "outputId": "92f97183-d798-46c2-d3c9-81095ae23815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.27.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Collecting sentencepiece (from torchtext==0.6.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.1\n",
            "    Uninstalling torchtext-0.15.1:\n",
            "      Successfully uninstalled torchtext-0.15.1\n",
            "Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=669b565bfcfd26fd71040aeeeeb10204eb6ac1d66b34a6b344dfc817801f3261\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "Successfully installed gdown-4.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.6.0\n",
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "! pip install wget\n",
        "! pip install gdown\n",
        "! pip install --upgrade gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "! wandb login 519ef73bbeeba4f437e82d8aeb9cf27e62a84740"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87vvJG0Pro7B",
        "outputId": "8303690b-6f70-447e-e0a0-9dcd99defc84"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.22.2-py2.py3-none-any.whl (203 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.3/203.3 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=271a40dcca1cc23c4e6f4ff084ec805ba1fb9d03621db55baa65ef4ddb01acf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.22.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wYmZkdTHV1gb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import gdown\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "from io import open\n",
        "import string, time, math\n",
        "import wget\n",
        "from zipfile import ZipFile\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "# import numpy as np\n",
        "import spacy\n",
        "# import random\n",
        "# from torch.utils.tensorboard import SummaryWriter # to print to tensorboard\n",
        "# from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gaw2m7L5Dj0j"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "# CUDA\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic=True\n",
        "torch.backends.cudnn.benchmark=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q87jZlq8D--z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a323f382-7eda-4171-a28b-375218ac959f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw\n",
            "To: /content/aksharantar_sampled.zip\n",
            "100%|██████████| 14.0M/14.0M [00:00<00:00, 71.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aksharantar_sampled.zip\n",
            "Extracting files...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Getting the Dataset\n",
        "url = 'https://drive.google.com/uc?id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw&export=download'\n",
        "\n",
        "if not os.path.exists(\"aksharantar_sampled\"):\n",
        "  filename = gdown.download(url = url, quiet=False, fuzzy=True)\n",
        "  print(filename)\n",
        "  with ZipFile(filename, 'r') as z:\n",
        "    print('Extracting files...')\n",
        "    z.extractall()\n",
        "    print('Done!')\n",
        "  os.remove(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1cP5vkMgOD3",
        "outputId": "369197e5-8442-4136-f127-f06d61a647e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<PAD>': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
          ]
        }
      ],
      "source": [
        "eng_alpha = 'abcdefghijklmnopqrstuvwxyz'\n",
        "pad_char = '<PAD>'\n",
        "\n",
        "eng_alpha2idx = {pad_char: 0}\n",
        "for index, alpha in enumerate(eng_alpha):\n",
        "  eng_alpha2idx[alpha] = index+1\n",
        "\n",
        "print(eng_alpha2idx)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change Indic Language here\n",
        "indic_lang = 'ben'\n",
        "# indic_lang = 'hin'"
      ],
      "metadata": {
        "id": "O_Jxy7gDr4RX"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beH3ItF4gTTd",
        "outputId": "9bf755cf-0570-42cf-e33c-34134d0f9204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ঀ', 'ঁ', 'ং', 'ঃ', '\\u0984', 'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'ঌ', '\\u098d', '\\u098e', 'এ', 'ঐ', '\\u0991', '\\u0992', 'ও', 'ঔ', 'ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', 'ঝ', 'ঞ', 'ট', 'ঠ', 'ড', 'ঢ', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন', '\\u09a9', 'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', '\\u09b1', 'ল', '\\u09b3', '\\u09b4', '\\u09b5', 'শ', 'ষ', 'স', 'হ', '\\u09ba', '\\u09bb', '়', 'ঽ', 'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ৄ', '\\u09c5', '\\u09c6', 'ে', 'ৈ', '\\u09c9', '\\u09ca', 'ো', 'ৌ', '্', 'ৎ', '\\u09cf', '\\u09d0', '\\u09d1', '\\u09d2', '\\u09d3', '\\u09d4', '\\u09d5', '\\u09d6', 'ৗ', '\\u09d8', '\\u09d9', '\\u09da', '\\u09db', 'ড়', 'ঢ়', '\\u09de', 'য়', 'ৠ', 'ৡ', 'ৢ', 'ৣ', '\\u09e4', '\\u09e5', '০', '১', '২', '৩', '৪', '৫', '৬', '৭', '৮', '৯', 'ৰ', 'ৱ', '৲', '৳', '৴', '৵', '৶', '৷', '৸', '৹', '৺', '৻', 'ৼ', '৽', '৾']\n",
            "{'<PAD>': 0, 'ঀ': 1, 'ঁ': 2, 'ং': 3, 'ঃ': 4, '\\u0984': 5, 'অ': 6, 'আ': 7, 'ই': 8, 'ঈ': 9, 'উ': 10, 'ঊ': 11, 'ঋ': 12, 'ঌ': 13, '\\u098d': 14, '\\u098e': 15, 'এ': 16, 'ঐ': 17, '\\u0991': 18, '\\u0992': 19, 'ও': 20, 'ঔ': 21, 'ক': 22, 'খ': 23, 'গ': 24, 'ঘ': 25, 'ঙ': 26, 'চ': 27, 'ছ': 28, 'জ': 29, 'ঝ': 30, 'ঞ': 31, 'ট': 32, 'ঠ': 33, 'ড': 34, 'ঢ': 35, 'ণ': 36, 'ত': 37, 'থ': 38, 'দ': 39, 'ধ': 40, 'ন': 41, '\\u09a9': 42, 'প': 43, 'ফ': 44, 'ব': 45, 'ভ': 46, 'ম': 47, 'য': 48, 'র': 49, '\\u09b1': 50, 'ল': 51, '\\u09b3': 52, '\\u09b4': 53, '\\u09b5': 54, 'শ': 55, 'ষ': 56, 'স': 57, 'হ': 58, '\\u09ba': 59, '\\u09bb': 60, '়': 61, 'ঽ': 62, 'া': 63, 'ি': 64, 'ী': 65, 'ু': 66, 'ূ': 67, 'ৃ': 68, 'ৄ': 69, '\\u09c5': 70, '\\u09c6': 71, 'ে': 72, 'ৈ': 73, '\\u09c9': 74, '\\u09ca': 75, 'ো': 76, 'ৌ': 77, '্': 78, 'ৎ': 79, '\\u09cf': 80, '\\u09d0': 81, '\\u09d1': 82, '\\u09d2': 83, '\\u09d3': 84, '\\u09d4': 85, '\\u09d5': 86, '\\u09d6': 87, 'ৗ': 88, '\\u09d8': 89, '\\u09d9': 90, '\\u09da': 91, '\\u09db': 92, 'ড়': 93, 'ঢ়': 94, '\\u09de': 95, 'য়': 96, 'ৠ': 97, 'ৡ': 98, 'ৢ': 99, 'ৣ': 100, '\\u09e4': 101, '\\u09e5': 102, '০': 103, '১': 104, '২': 105, '৩': 106, '৪': 107, '৫': 108, '৬': 109, '৭': 110, '৮': 111, '৯': 112, 'ৰ': 113, 'ৱ': 114, '৲': 115, '৳': 116, '৴': 117, '৵': 118, '৶': 119, '৷': 120, '৸': 121, '৹': 122, '৺': 123, '৻': 124, 'ৼ': 125, '৽': 126, '৾': 127}\n"
          ]
        }
      ],
      "source": [
        "# Bengali Unicode Hex Range: 2432-2558\n",
        "# Hindi Unicode Hex Range: 2304-2431\n",
        "\n",
        "min_range = 2304\n",
        "max_range = 2431\n",
        "\n",
        "if indic_lang == 'ben':\n",
        "  min_range = 2432\n",
        "  max_range = 2558\n",
        "elif indic_lang == 'hin':\n",
        "  min_range = 2304\n",
        "  max_range = 2431\n",
        "\n",
        "indic_alpha = [chr(alpha) for alpha in range(min_range, max_range + 1)]\n",
        "print(indic_alpha)\n",
        "indic_alpha_size = len(indic_alpha)\n",
        "\n",
        "indic_alpha2idx = {pad_char: 0}\n",
        "for index, alpha in enumerate(indic_alpha):\n",
        "  indic_alpha2idx[alpha] = index+1\n",
        "\n",
        "print(indic_alpha2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "tNrcqaQQ6j-5"
      },
      "outputs": [],
      "source": [
        "indic_idx2alpha = {v: k for k, v in indic_alpha2idx.items()}\n",
        "eng_idx2alpha = {v: k for k, v in eng_alpha2idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "3j6vZGlyEuKo"
      },
      "outputs": [],
      "source": [
        "def tokenize_indic(string):\n",
        "  # return string.split()\n",
        "  char_list =  [*string]\n",
        "  char_list = [indic_alpha2idx[char] for char in char_list]\n",
        "  return char_list\n",
        "\n",
        "def tokenize_eng(string):\n",
        "  # return string.split()\n",
        "  char_list =  [*string]\n",
        "  char_list = [eng_alpha2idx[char] for char in char_list]\n",
        "  return char_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "FCuwdH7VHXE1"
      },
      "outputs": [],
      "source": [
        "# importing python package\n",
        "import pandas as pd\n",
        "  \n",
        "file_names = ['test', 'train', 'valid']\n",
        "\n",
        "for index, file_name in enumerate(file_names):\n",
        "  # read contents of csv file\n",
        "  file = pd.read_csv(f'aksharantar_sampled/{indic_lang}/{indic_lang}_{file_name}.csv')\n",
        "    \n",
        "  # adding header\n",
        "  headerList = ['eng', f'{indic_lang}']\n",
        "    \n",
        "  # converting data frame to csv\n",
        "  file.to_csv(f'aksharantar_sampled/{indic_lang}/{indic_lang}_{file_name}.csv', header=headerList, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "gUYK7Us1EDqD"
      },
      "outputs": [],
      "source": [
        "eng = Field(sequential=True, use_vocab=True, tokenize=tokenize_eng, init_token='<sos>', eos_token='<eos>')\n",
        "indic = Field(sequential=True, use_vocab=True, tokenize=tokenize_indic, init_token='<sos>', eos_token='<eos>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5RFKlw5MEx6l"
      },
      "outputs": [],
      "source": [
        "fields={'eng': ('eng', eng), f'{indic_lang}': ('indic', indic)}\n",
        "\n",
        "path_name = f'aksharantar_sampled/{indic_lang}'\n",
        "train_name = f'{indic_lang}_train.csv'\n",
        "val_name = f'{indic_lang}_valid.csv'\n",
        "test_name = f'{indic_lang}_test.csv'\n",
        "train_data, val_data, test_data = TabularDataset.splits(\n",
        "    path= path_name,\n",
        "    train=train_name,\n",
        "    validation=val_name,\n",
        "    test=test_name,\n",
        "    format='csv',\n",
        "    fields=fields\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwT-AtD0MaKK",
        "outputId": "0e23dd2a-230a-48b2-b049-fc0ca1d01daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['eng', 'indic'])\n"
          ]
        }
      ],
      "source": [
        "print(train_data[0].__dict__.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIaEB78IKrdL",
        "outputId": "2d3a853b-6de8-412e-a23b-2ae5b470b39d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_values([[2, 9, 14, 4, 8, 25, 1], [45, 64, 41, 78, 39, 78, 48, 63]])\n"
          ]
        }
      ],
      "source": [
        "print(train_data[0].__dict__.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "_9owJfZJMFJq"
      },
      "outputs": [],
      "source": [
        "eng.build_vocab(train_data, max_size = 1000, min_freq = 1)\n",
        "indic.build_vocab(train_data, max_size = 1000, min_freq = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "NmKmTPyty7mI"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(net, device = 'cpu', data = val_data):\n",
        "    # net = net.eval().to(device)\n",
        "    # predictions = []\n",
        "    accuracy = 0\n",
        "    count = 0\n",
        "    for i in range(len(data)):\n",
        "        eng_word, indic_word = [j for j in data[i].__dict__.values()]\n",
        "        # gt = gt_rep(indic_word, indic_alpha2idx, device)\n",
        "\n",
        "        # outputs = infer(net, eng_word, gt.shape[0], device)\n",
        "        output = translit_infer(net, eng_word, eng, indic, device, max_length=50)\n",
        "        correct = 0\n",
        "\n",
        "        for index, char in output:\n",
        "          if char == indic_word[index]:\n",
        "            correct += 1\n",
        "\n",
        "\n",
        "        char_level_acc = correct/len(indic_word)\n",
        "        \n",
        "        if char_level_acc == 1.0:\n",
        "          count += 1\n",
        "    # print(count)\n",
        "    accuracy = count/len(data)\n",
        "    \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "8qyO9m_DcJAW"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True, dropout=p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "        encoder_states, (hidden, cell) = self.rnn(embedding)\n",
        "\n",
        "        hidden = (hidden[0:1] + hidden[1:2])/2\n",
        "        cell = (cell[0:1] + cell[1:2])/2\n",
        "\n",
        "        return encoder_states, hidden, cell\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        # hidden_size*2 comes from the encoder\n",
        "        self.rnn = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "        # We would take the hidden states from the encoder, but we would also take the hidden state from the decoder (s(t-1), h(j))\n",
        "        self.energy = nn.Linear(hidden_size*3, 1)\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, encoder_states, hidden, cell):\n",
        "        # x: (1, N) where N is the batch size\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding shape: (1, N, embedding_size)\n",
        "\n",
        "        sequence_length = encoder_states.shape[0]\n",
        "        # print(f'hidden 0 -> {hidden[0].shape}')\n",
        "        # To add them together, we need to have the same dim along that axis\n",
        "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
        "        # h_reshaped: (seq_length, N, hidden_size*2)\n",
        "        # h_reshape -> (hidden_size), encoder_states -> (hidden_size*2)\n",
        "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim = 2)))\n",
        "#         energy = self.relu(self.energy(encoder_states))\n",
        "\n",
        "        # Compute attention values\n",
        "        attention = self.softmax(energy)\n",
        "        # (seq_length, N, 1) -> We'll normalize through the first dim - seq_length so that sum of attention scores equals 1 for a specific batch\n",
        "\n",
        "        # Goal is to elementwise multiply attention with the encoder state\n",
        "        attention = attention.permute(1,2,0) #-> changing order to (N,seq_length,1)\n",
        "        encoder_states = encoder_states.permute(1,0,2) #-> changer order to (N, seq_length, hidden_size*2)\n",
        "\n",
        "#         # Multiplying the above by torch.bmm we'll get: (N, 1, hidden_size*2) -> permuting it -> (1,N, hidden_size*2)\n",
        "        context_vector = torch.bmm(attention, encoder_states).permute(1,0,2)\n",
        "\n",
        "\n",
        "        # attention: (seq_length, N, 1), snk\n",
        "        # encoder_states: (seq_length, N, hidden_size*2), snl\n",
        "        # we want context_vector: (1, N, hidden_size*2), i.e knl\n",
        "#         context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
        "\n",
        "        #context_vector is for a particular timestep for decoder, since decoding one word at a time\n",
        "        # we'll concat along 3rd dim to get hidden_size*3 -> concatenating attention vector and embedding input\n",
        "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
        "\n",
        "        #Send this through RNN\n",
        "        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        # outputs shape: (1, N, hidden_size)\n",
        "\n",
        "        predictions = self.fc(outputs)\n",
        "\n",
        "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        predictions = predictions.squeeze(0)\n",
        "\n",
        "        return predictions, hidden, cell\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(indic.vocab)\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        encoder_states, hidden, cell = self.encoder(source)\n",
        "\n",
        "        # Grab the first input to the Decoder which will be <SOS> token\n",
        "        x = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            # print(hidden.shape, cell.shape)\n",
        "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            # With probability of teacher_force_ratio we take the actual next word\n",
        "            # otherwise we take the word that the Decoder predicted it to be.\n",
        "            # Teacher Forcing is used so that the model gets used to seeing\n",
        "            # similar inputs at training and testing time, if teacher forcing is 1\n",
        "            # then inputs at test time might be completely different than what the\n",
        "            # network is used to. This was a long comment.\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "        return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "NJBBW4u3zwae"
      },
      "outputs": [],
      "source": [
        "def translit_infer(model, word, eng, indic, device, max_length=50):\n",
        "    tokens = tokenize_eng(word)\n",
        "\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\n",
        "    tokens.insert(0, eng.init_token)\n",
        "    tokens.append(eng.eos_token)\n",
        "\n",
        "\n",
        "    text_to_indices = [eng.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    # Convert to Tensor\n",
        "    word_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        encoder_states, hidden, cell = model.encoder(word_tensor)\n",
        "\n",
        "    outputs = [indic.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_char = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_char, encoder_states, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == indic.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translit_res = [indic.vocab.itos[idx] for idx in outputs]\n",
        "\n",
        "    # remove start token\n",
        "    translit_res_word = ''\n",
        "    translit_res = translit_res[1:]\n",
        "    # return translit_res\n",
        "    for i in translit_res:\n",
        "      if i != \"<eos>\":\n",
        "        translit_res_word += indic_idx2alpha[i]\n",
        "      else:\n",
        "        break\n",
        "    return translit_res_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "8yUgp84Mz3t_"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=f\"{indic_lang}_2_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "EHVn3O5_ITh8"
      },
      "outputs": [],
      "source": [
        "from IPython.utils.path import target_outdated\n",
        "def check_accuracy(loader, model, input_shape=None, toggle_eval=True, print_accuracy=True):\n",
        "    if toggle_eval:\n",
        "        model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loader.create_batches()\n",
        "        for batch in loader.batches:\n",
        "          for example in batch:\n",
        "            num_samples += 1\n",
        "            eng_word = \"\".join([eng_idx2alpha[val] for val in example.eng])\n",
        "            indic_word = \"\".join([indic_idx2alpha[val2] for val2 in example.indic])\n",
        "            indic_pred = translit_infer(model, eng_word, eng, indic, device, max_length=50)\n",
        "            \n",
        "            if indic_pred == indic_word:\n",
        "              num_correct += 1\n",
        "\n",
        "    accuracy = num_correct / num_samples\n",
        "    if toggle_eval:\n",
        "        model.train()\n",
        "#     if print_accuracy:\n",
        "#         print(f\"Accuracy on validation set: {accuracy * 100:.2f}%\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "FqtOKyDGm7Oh"
      },
      "outputs": [],
      "source": [
        "### Now model is ready to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "EzVm5W-0qfrN"
      },
      "outputs": [],
      "source": [
        "# Training Hyperparameters\n",
        "num_epochs =30\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "# Model Hyperparameters\n",
        "load_model = False\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "input_size_encoder = len(eng.vocab)\n",
        "input_size_decoder = len(indic.vocab)\n",
        "output_size = len(indic.vocab)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "rggL-b4Mm-4w"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    \n",
        "    train_iterator, val_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size = batch_size,\n",
        "    # Examples of similar length will be in same batch to minimize padding and save on compute\n",
        "    sort_within_batch = True,\n",
        "    sort_key = lambda x: len(x.eng),\n",
        "    device = device)\n",
        "\n",
        "\n",
        "\n",
        "    encoder_net = Encoder(\n",
        "        input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
        "        ).to(device)\n",
        "    decoder_net = Decoder(\n",
        "        input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout\n",
        "        ).to(device)\n",
        "\n",
        "    model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    pad_idx = indic.vocab.stoi['<pad>']\n",
        "    # if all examples in batch are of similar length, don't incur penalty for this padding\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
        "\n",
        "    if load_model:\n",
        "      load_checkpoint(torch.load(f'{indic_lang}_checkpoint.pth.tar'), model, optimizer)\n",
        "    if indic_lang == 'hin':\n",
        "      word = 'bachta'\n",
        "      og_translit = 'बचता'\n",
        "    elif indic_lang == 'ben':\n",
        "      word = 'gabhaaskar'\n",
        "      og_translit = 'গাভাস্কার'\n",
        "    acc_val_prev = 0\n",
        "    acc_val_current = 0\n",
        "    for epoch in range(num_epochs):\n",
        "      print(f'Epoch [{epoch+1} / {num_epochs}]')\n",
        "\n",
        "      checkpoint = {\n",
        "          'state_dict': model.state_dict(),\n",
        "          'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "      if acc_val_current > acc_val_prev:\n",
        "\n",
        "          if os.path.exists(f'{indic_lang}_checkpoint_new_{acc_val_prev*100:.2f}.pth.tar'):\n",
        "              os.remove(f'{indic_lang}_checkpoint_new_{acc_val_prev*100:.2f}.pth.tar')\n",
        "          acc_val_prev = acc_val_current\n",
        "          save_checkpoint(checkpoint, f'{indic_lang}_checkpoint_new_{acc_val_current*100:.2f}.pth.tar')\n",
        "\n",
        "\n",
        "\n",
        "      loop = tqdm(enumerate(train_iterator), total=len(train_iterator))\n",
        "      for batch_idx, batch in loop:\n",
        "        inp_data = batch.eng.to(device)\n",
        "        target = batch.indic.to(device)\n",
        "\n",
        "        output = model(inp_data, target)\n",
        "        # output shape: (target_len, batch_size, output_dim)\n",
        "\n",
        "        #basically reshape output keeping last output_dim same\n",
        "        output = output[1:].reshape(-1, output.shape[2]) # so that first start token is not sent to out model\n",
        "        # target -> (target_len, batch_size)\n",
        "        target = target[1:].reshape(-1)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # to avoid exploding gradients, clip them when they are above a threshold\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        optimizer.step()\n",
        "\n",
        "      model.eval() # turns off Dropout\n",
        "      translit_res = translit_infer(model, word, eng, indic, device, max_length=50)\n",
        "      print(f'Translated example word:  English: {word}, Actual: {og_translit}, Predicted: {translit_res}')\n",
        "      model.train()\n",
        "\n",
        "      print('Computing Loss and Validation Accuracy...')\n",
        "      acc_val_current = check_accuracy(val_iterator, model, input_shape=None, toggle_eval=True, print_accuracy=True)\n",
        "      print(f'Training Loss: {loss.item()}, Validation Accuracy: {acc_val_current * 100:.2f}%')\n",
        "      print('--------------------------')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czpOpuQZs4Qk",
        "outputId": "6df42a6a-2fd8-46aa-b359-6afa7ec530f5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1 / 30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 30.14it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গাভাসকার\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 1.0853245258331299, Validation Accuracy: 19.93%\n",
            "--------------------------\n",
            "Epoch [2 / 30]\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 30.28it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাসকার\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.6437900066375732, Validation Accuracy: 27.06%\n",
            "--------------------------\n",
            "Epoch [3 / 30]\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 29.91it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গাভাস্কার\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.5041789412498474, Validation Accuracy: 29.99%\n",
            "--------------------------\n",
            "Epoch [4 / 30]\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:27<00:00, 29.16it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাসকার\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.6932497620582581, Validation Accuracy: 31.33%\n",
            "--------------------------\n",
            "Epoch [5 / 30]\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:27<00:00, 28.94it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গাভাসকার\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.5923864841461182, Validation Accuracy: 32.92%\n",
            "--------------------------\n",
            "Epoch [6 / 30]\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:28<00:00, 28.23it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাসকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.3565860390663147, Validation Accuracy: 34.24%\n",
            "--------------------------\n",
            "Epoch [7 / 30]\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:27<00:00, 29.47it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গাভাস্কর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.35583800077438354, Validation Accuracy: 34.09%\n",
            "--------------------------\n",
            "Epoch [8 / 30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:27<00:00, 29.58it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাসসকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.252864807844162, Validation Accuracy: 35.16%\n",
            "--------------------------\n",
            "Epoch [9 / 30]\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:27<00:00, 29.32it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গাভাসকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.44287312030792236, Validation Accuracy: 35.04%\n",
            "--------------------------\n",
            "Epoch [10 / 30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 30.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাসকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.24950850009918213, Validation Accuracy: 35.82%\n",
            "--------------------------\n",
            "Epoch [11 / 30]\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 30.03it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাসকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.2005794644355774, Validation Accuracy: 35.82%\n",
            "--------------------------\n",
            "Epoch [12 / 30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 30.47it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাষসকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.31307682394981384, Validation Accuracy: 35.82%\n",
            "--------------------------\n",
            "Epoch [13 / 30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:27<00:00, 29.50it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাসকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.30612796545028687, Validation Accuracy: 34.87%\n",
            "--------------------------\n",
            "Epoch [14 / 30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 29.77it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গাভাসকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.22051279246807098, Validation Accuracy: 36.90%\n",
            "--------------------------\n",
            "Epoch [15 / 30]\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:27<00:00, 29.60it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাষাকরর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.15640075504779816, Validation Accuracy: 37.12%\n",
            "--------------------------\n",
            "Epoch [16 / 30]\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:27<00:00, 29.45it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাষ্কর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.4814348518848419, Validation Accuracy: 36.95%\n",
            "--------------------------\n",
            "Epoch [17 / 30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 29.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাষার্কর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.17639507353305817, Validation Accuracy: 36.46%\n",
            "--------------------------\n",
            "Epoch [18 / 30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 30.01it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাসকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.15480796992778778, Validation Accuracy: 37.24%\n",
            "--------------------------\n",
            "Epoch [19 / 30]\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 29.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাসকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.08799704164266586, Validation Accuracy: 36.73%\n",
            "--------------------------\n",
            "Epoch [20 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 29.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাষ্কার\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.3231281042098999, Validation Accuracy: 36.17%\n",
            "--------------------------\n",
            "Epoch [21 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 30.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাষ্কর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.1509305089712143, Validation Accuracy: 36.68%\n",
            "--------------------------\n",
            "Epoch [22 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 29.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাষ্কর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.128634974360466, Validation Accuracy: 36.87%\n",
            "--------------------------\n",
            "Epoch [23 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 30.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাষ্কার\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.12549835443496704, Validation Accuracy: 37.09%\n",
            "--------------------------\n",
            "Epoch [24 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 29.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাষ্কর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.1306561827659607, Validation Accuracy: 37.22%\n",
            "--------------------------\n",
            "Epoch [25 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 29.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাসকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.08809719234704971, Validation Accuracy: 36.46%\n",
            "--------------------------\n",
            "Epoch [26 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 29.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাষকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.15128058195114136, Validation Accuracy: 37.07%\n",
            "--------------------------\n",
            "Epoch [27 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 29.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাষ্কর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.11685103923082352, Validation Accuracy: 37.19%\n",
            "--------------------------\n",
            "Epoch [28 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 30.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাষ্কর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.08528614789247513, Validation Accuracy: 37.19%\n",
            "--------------------------\n",
            "Epoch [29 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 29.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গভাসকর\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.08410114049911499, Validation Accuracy: 36.92%\n",
            "--------------------------\n",
            "Epoch [30 / 30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:26<00:00, 29.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated example word:  English: gabhaaskar, Actual: গাভাস্কার, Predicted: গাভাসকার\n",
            "Computing Loss and Validation Accuracy...\n",
            "Training Loss: 0.2135738879442215, Validation Accuracy: 36.80%\n",
            "--------------------------\n",
            "CPU times: user 27min 7s, sys: 7.99 s, total: 27min 15s\n",
            "Wall time: 27min 27s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uORgcL9KM2n_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}