{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KreNpqQWnde",
    "outputId": "06801c0e-708c-4731-c724-fb88b6aefb55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.6.0 in /home/oikantik/anaconda3/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: requests in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torchtext==0.6.0) (2.27.1)\n",
      "Requirement already satisfied: numpy in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torchtext==0.6.0) (1.21.5)\n",
      "Requirement already satisfied: sentencepiece in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torchtext==0.6.0) (0.1.99)\n",
      "Requirement already satisfied: tqdm in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torchtext==0.6.0) (4.64.0)\n",
      "Requirement already satisfied: six in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torchtext==0.6.0) (1.16.0)\n",
      "Requirement already satisfied: torch in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torchtext==0.6.0) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.6.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.6.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.6.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.6.0) (2.0.4)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (8.5.0.96)\n",
      "Requirement already satisfied: filelock in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (3.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (4.5.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (11.4.0.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (11.7.101)\n",
      "Requirement already satisfied: networkx in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (2.11.3)\n",
      "Requirement already satisfied: sympy in /home/oikantik/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.6.0) (1.10.1)\n",
      "Requirement already satisfied: wheel in /home/oikantik/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->torchtext==0.6.0) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /home/oikantik/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->torchtext==0.6.0) (61.2.0)\n",
      "Requirement already satisfied: lit in /home/oikantik/anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.0)\n",
      "Requirement already satisfied: cmake in /home/oikantik/anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from jinja2->torch->torchtext==0.6.0) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from sympy->torch->torchtext==0.6.0) (1.2.1)\n",
      "Requirement already satisfied: wget in /home/oikantik/anaconda3/lib/python3.9/site-packages (3.2)\n",
      "Requirement already satisfied: gdown in /home/oikantik/anaconda3/lib/python3.9/site-packages (4.7.1)\n",
      "Requirement already satisfied: tqdm in /home/oikantik/anaconda3/lib/python3.9/site-packages (from gdown) (4.64.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: six in /home/oikantik/anaconda3/lib/python3.9/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: filelock in /home/oikantik/anaconda3/lib/python3.9/site-packages (from gdown) (3.6.0)\n",
      "Requirement already satisfied: requests[socks] in /home/oikantik/anaconda3/lib/python3.9/site-packages (from gdown) (2.27.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: gdown in /home/oikantik/anaconda3/lib/python3.9/site-packages (4.7.1)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef32fcfbb0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/gdown/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef32fcf910>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/gdown/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fef32fcfa00>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/gdown/\u001b[0m\n",
      "Requirement already satisfied: filelock in /home/oikantik/anaconda3/lib/python3.9/site-packages (from gdown) (3.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: tqdm in /home/oikantik/anaconda3/lib/python3.9/site-packages (from gdown) (4.64.0)\n",
      "Requirement already satisfied: requests[socks] in /home/oikantik/anaconda3/lib/python3.9/site-packages (from gdown) (2.27.1)\n",
      "Requirement already satisfied: six in /home/oikantik/anaconda3/lib/python3.9/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.14)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/oikantik/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.6.0\n",
    "# import locale\n",
    "# locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "! pip install wget\n",
    "! pip install gdown\n",
    "! pip install --upgrade gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wYmZkdTHV1gb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 10:58:11.439280: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-15 10:58:12.279854: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-15 10:58:12.279946: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-15 10:58:12.279956: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import gdown\n",
    "from tqdm import tqdm\n",
    "# import wandb\n",
    "from io import open\n",
    "import string, time, math\n",
    "import wget\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "# from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "# import numpy as np\n",
    "import spacy\n",
    "# import random\n",
    "# from torch.utils.tensorboard import SummaryWriter # to print to tensorboard\n",
    "# from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gaw2m7L5Dj0j"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# CUDA\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic=True\n",
    "# torch.backends.cudnn.benchmark=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "q87jZlq8D--z"
   },
   "outputs": [],
   "source": [
    "# Getting the Dataset\n",
    "url = 'https://drive.google.com/uc?id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw&export=download'\n",
    "# filename = os.path.basename(url)\n",
    "# print(filename)\n",
    "\n",
    "if not os.path.exists(\"aksharantar_sampled\"):\n",
    "  filename = gdown.download(url = url, quiet=False, fuzzy=True)\n",
    "  print(filename)\n",
    "  with ZipFile(filename, 'r') as z:\n",
    "    print('Extracting files...')\n",
    "    z.extractall()\n",
    "    print('Done!')\n",
    "  os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1cP5vkMgOD3",
    "outputId": "b57b6328-3e95-4dd0-9c9c-f46b35bc69ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "eng_alpha = 'abcdefghijklmnopqrstuvwxyz'\n",
    "pad_char = '<PAD>'\n",
    "\n",
    "eng_alpha2idx = {pad_char: 0}\n",
    "for index, alpha in enumerate(eng_alpha):\n",
    "  eng_alpha2idx[alpha] = index+1\n",
    "\n",
    "print(eng_alpha2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beH3ItF4gTTd",
    "outputId": "72fe6c23-c7e3-4dd5-8504-0605cfcfb771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ऀ', 'ँ', 'ं', 'ः', 'ऄ', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऌ', 'ऍ', 'ऎ', 'ए', 'ऐ', 'ऑ', 'ऒ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'ऩ', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल', 'ळ', 'ऴ', 'व', 'श', 'ष', 'स', 'ह', 'ऺ', 'ऻ', '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'ॅ', 'ॆ', 'े', 'ै', 'ॉ', 'ॊ', 'ो', 'ौ', '्', 'ॎ', 'ॏ', 'ॐ', '॑', '॒', '॓', '॔', 'ॕ', 'ॖ', 'ॗ', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', 'य़', 'ॠ', 'ॡ', 'ॢ', 'ॣ', '।', '॥', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '॰', 'ॱ', 'ॲ', 'ॳ', 'ॴ', 'ॵ', 'ॶ', 'ॷ', 'ॸ', 'ॹ', 'ॺ', 'ॻ', 'ॼ', 'ॽ', 'ॾ', 'ॿ']\n",
      "{'<PAD>': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n"
     ]
    }
   ],
   "source": [
    "# Bengali Unicode Hex Range: 2432-2558\n",
    "# Hindi Unicode Hex Range: 2304-2431\n",
    "\n",
    "min_range = 2304\n",
    "max_range = 2431\n",
    "\n",
    "# if indic_lang == 'ben':\n",
    "#   min_range = 2432\n",
    "#   max_range = 2558\n",
    "# elif indic_lang == 'hindi':\n",
    "#   min_range = 2304\n",
    "#   max_range = 2431\n",
    "\n",
    "indic_alpha = [chr(alpha) for alpha in range(min_range, max_range + 1)]\n",
    "print(indic_alpha)\n",
    "indic_alpha_size = len(indic_alpha)\n",
    "\n",
    "indic_alpha2idx = {pad_char: 0}\n",
    "for index, alpha in enumerate(indic_alpha):\n",
    "  indic_alpha2idx[alpha] = index+1\n",
    "\n",
    "print(indic_alpha2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tNrcqaQQ6j-5"
   },
   "outputs": [],
   "source": [
    "indic_idx2alpha = {v: k for k, v in indic_alpha2idx.items()}\n",
    "eng_idx2alpha = {v: k for k, v in eng_alpha2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3j6vZGlyEuKo"
   },
   "outputs": [],
   "source": [
    "def tokenize_indic(string):\n",
    "  # return string.split()\n",
    "  char_list =  [*string]\n",
    "  char_list = [indic_alpha2idx[char] for char in char_list]\n",
    "  return char_list\n",
    "\n",
    "def tokenize_eng(string):\n",
    "  # return string.split()\n",
    "  char_list =  [*string]\n",
    "  char_list = [eng_alpha2idx[char] for char in char_list]\n",
    "  return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYEkyS3KLRh1",
    "outputId": "771a478e-5377-4f1b-d003-0db86a4e7522"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45, 64, 41, 78, 39, 78, 48, 63]\n",
      "[8, 5, 12, 12, 15]\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_indic('बिन्द्या'))\n",
    "print(tokenize_eng('hello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0PulDBIPFWhj"
   },
   "outputs": [],
   "source": [
    "# Change Indic Language here\n",
    "# indic_lang = 'ben'\n",
    "indic_lang = 'hin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FCuwdH7VHXE1"
   },
   "outputs": [],
   "source": [
    "# importing python package\n",
    "import pandas as pd\n",
    "  \n",
    "file_names = ['test', 'train', 'valid']\n",
    "\n",
    "for index, file_name in enumerate(file_names):\n",
    "  # read contents of csv file\n",
    "  file = pd.read_csv(f'aksharantar_sampled/{indic_lang}/{indic_lang}_{file_name}.csv')\n",
    "  # print(\"\\nOriginal file:\")\n",
    "  # print(file)\n",
    "    \n",
    "  # adding header\n",
    "  headerList = ['eng', f'{indic_lang}']\n",
    "    \n",
    "  # converting data frame to csv\n",
    "  file.to_csv(f'aksharantar_sampled/{indic_lang}/{indic_lang}_{file_name}.csv', header=headerList, index=False)\n",
    "    \n",
    "  # display modified csv file\n",
    "  # file2 = pd.read_csv(f'aksharantar_sampled/{indic_lang}/{indic_lang}_valid_2.csv')\n",
    "  # print('\\nModified file:')\n",
    "  # print(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gUYK7Us1EDqD"
   },
   "outputs": [],
   "source": [
    "eng = Field(sequential=True, use_vocab=True, tokenize=tokenize_eng, init_token='<sos>', eos_token='<eos>')\n",
    "indic = Field(sequential=True, use_vocab=True, tokenize=tokenize_indic, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tig4Kqp0MUsd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5RFKlw5MEx6l"
   },
   "outputs": [],
   "source": [
    "fields={'eng': ('eng', eng), f'{indic_lang}': ('indic', indic)}\n",
    "\n",
    "path_name = f'aksharantar_sampled/{indic_lang}'\n",
    "train_name = f'{indic_lang}_train.csv'\n",
    "val_name = f'{indic_lang}_valid.csv'\n",
    "test_name = f'{indic_lang}_test.csv'\n",
    "train_data, val_data, test_data = TabularDataset.splits(\n",
    "    path= path_name,\n",
    "    train=train_name,\n",
    "    validation=val_name,\n",
    "    test=test_name,\n",
    "    format='csv',\n",
    "    fields=fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AwT-AtD0MaKK",
    "outputId": "16e7ac88-caa4-44af-d547-800e191afbf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eng', 'indic'])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SIaEB78IKrdL",
    "outputId": "5384ea01-792f-4175-a090-c03a4aad461d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([[2, 9, 14, 4, 8, 25, 1], [45, 64, 41, 78, 39, 78, 48, 63]])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].__dict__.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iOx6Q3ZeAx6p",
    "outputId": "5d7049cd-e605-4986-991c-7470cc0f65f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 9, 14, 4, 8, 25, 1]\n",
      "b\n",
      "i\n",
      "n\n",
      "d\n",
      "h\n",
      "y\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "eng_w, indic_w = [i for i in train_data[0].__dict__.values()]\n",
    "print(eng_w)\n",
    "for val in eng_w:\n",
    "  print(eng_idx2alpha[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_9owJfZJMFJq"
   },
   "outputs": [],
   "source": [
    "eng.build_vocab(train_data, max_size = 1000, min_freq = 1)\n",
    "indic.build_vocab(train_data, max_size = 1000, min_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCoDONfrVmPY",
    "outputId": "90a03353-a104-40d2-8a13-863195f84bb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-IuQ5nHu2Ma",
    "outputId": "499f51ac-e47c-430e-b5e0-9a9569cc7153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['freqs', 'itos', 'unk_index', 'stoi', 'vectors'])\n",
      "Help on Vocab in module torchtext.vocab object:\n",
      "\n",
      "class Vocab(builtins.object)\n",
      " |  Vocab(counter, max_size=None, min_freq=1, specials=['<unk>', '<pad>'], vectors=None, unk_init=None, vectors_cache=None, specials_first=True)\n",
      " |  \n",
      " |  Defines a vocabulary object that will be used to numericalize a field.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      freqs: A collections.Counter object holding the frequencies of tokens\n",
      " |          in the data used to build the Vocab.\n",
      " |      stoi: A collections.defaultdict instance mapping token strings to\n",
      " |          numerical identifiers.\n",
      " |      itos: A list of token strings indexed by their numerical identifiers.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getitem__(self, token)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, counter, max_size=None, min_freq=1, specials=['<unk>', '<pad>'], vectors=None, unk_init=None, vectors_cache=None, specials_first=True)\n",
      " |      Create a Vocab object from a collections.Counter.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          counter: collections.Counter object holding the frequencies of\n",
      " |              each value found in the data.\n",
      " |          max_size: The maximum size of the vocabulary, or None for no\n",
      " |              maximum. Default: None.\n",
      " |          min_freq: The minimum frequency needed to include a token in the\n",
      " |              vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
      " |          specials: The list of special tokens (e.g., padding or eos) that\n",
      " |              will be prepended to the vocabulary. Default: ['<unk'>, '<pad>']\n",
      " |          vectors: One of either the available pretrained vectors\n",
      " |              or custom pretrained vectors (see Vocab.load_vectors);\n",
      " |              or a list of aforementioned vectors\n",
      " |          unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
      " |              to zero vectors; can be any function that takes in a Tensor and\n",
      " |              returns a Tensor of the same size. Default: torch.Tensor.zero_\n",
      " |          vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
      " |          specials_first: Whether to add special tokens into the vocabulary at first.\n",
      " |              If it is False, they are added into the vocabulary at last.\n",
      " |              Default: True.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  extend(self, v, sort=False)\n",
      " |  \n",
      " |  load_vectors(self, vectors, **kwargs)\n",
      " |      Arguments:\n",
      " |          vectors: one of or a list containing instantiations of the\n",
      " |              GloVe, CharNGram, or Vectors classes. Alternatively, one\n",
      " |              of or a list of available pretrained vectors:\n",
      " |              charngram.100d\n",
      " |              fasttext.en.300d\n",
      " |              fasttext.simple.300d\n",
      " |              glove.42B.300d\n",
      " |              glove.840B.300d\n",
      " |              glove.twitter.27B.25d\n",
      " |              glove.twitter.27B.50d\n",
      " |              glove.twitter.27B.100d\n",
      " |              glove.twitter.27B.200d\n",
      " |              glove.6B.50d\n",
      " |              glove.6B.100d\n",
      " |              glove.6B.200d\n",
      " |              glove.6B.300d\n",
      " |          Remaining keyword arguments: Passed to the constructor of Vectors classes.\n",
      " |  \n",
      " |  set_vectors(self, stoi, vectors, dim, unk_init=<method 'zero_' of 'torch._C._TensorBase' objects>)\n",
      " |      Set the vectors for the Vocab instance from a collection of Tensors.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          stoi: A dictionary of string to the index of the associated vector\n",
      " |              in the `vectors` input argument.\n",
      " |          vectors: An indexed iterable (or other structure supporting __getitem__) that\n",
      " |              given an input index, returns a FloatTensor representing the vector\n",
      " |              for the token associated with the index. For example,\n",
      " |              vector[stoi[\"string\"]] should return the vector for \"string\".\n",
      " |          dim: The dimensionality of the vectors.\n",
      " |          unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
      " |              to zero vectors; can be any function that takes in a Tensor and\n",
      " |              returns a Tensor of the same size. Default: torch.Tensor.zero_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  UNK = '<unk>'\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(eng.vocab.__dict__.keys())\n",
    "# print(eng.vocab.help?)\n",
    "print(help(eng.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDioMDH6uwsD",
    "outputId": "27379fcc-a8b1-4a5e-aeb7-e509859c8ea0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indic.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vX48JEqWa9hT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQAlqawP7PJ_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "9p1Gf44P7EWQ"
   },
   "outputs": [],
   "source": [
    "# indic_langs = sorted([indic_lang for indic_lang in os.listdir(\"aksharantar_sampled\") if indic_lang != '.DS_Store'])\n",
    "# print(indic_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dGRPlmpl7ELU"
   },
   "outputs": [],
   "source": [
    "# class TransLit_DataLoader(Dataset):\n",
    "#   def __init__(self, filename):\n",
    "#     self.eng_lang_words, self.indic_lang_words = self.readDataset(filename)\n",
    "#     self.shuffle_indices = list(range(len(self.eng_lang_words)))\n",
    "#     random.shuffle(self.shuffle_indices)\n",
    "#     self.shuffle_start_index = 0\n",
    "\n",
    "#   def __len__(self):\n",
    "#     return len(self.eng_lang_words)\n",
    "\n",
    "#   def __getitem__(self, idx):\n",
    "#     return self.eng_lang_words[idx], self.indic_lang_words[idx]\n",
    "\n",
    "#   def readDataset(self, filename):\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     # data = []\n",
    "\n",
    "#     with open(filename, 'r') as f:\n",
    "#       for line in f:\n",
    "#         line = line.split(',')\n",
    "#         eng_word = line[0].strip()\n",
    "#         indic_word = line[1].strip()\n",
    "#         X.append(eng_word)\n",
    "#         y.append(indic_word)\n",
    "#         # data_train.append((eng_word, indic_word))\n",
    "#     return X, y\n",
    "\n",
    "#   def get_random_sample(self):\n",
    "#     return self.__getitem__(np.random.randint(len(self.eng_lang_words)))\n",
    "\n",
    "#   def get_batch_from_array(self, batch_size, array):\n",
    "#     end = self.shuffle_start_index + batch_size\n",
    "#     batch = []\n",
    "#     if end >= len(self.eng_lang_words):\n",
    "#       batch = [array[i] for i in self.shuffle_indices[0:end%len(self.eng_lang_words)]]\n",
    "#     return batch + [array[i] for i in self.shuffle_indices[self.shuffle_start_index:end]]\n",
    "\n",
    "#   def get_batch(self, batch_size, postprocess = True):\n",
    "#     eng_lang_batch = self.get_batch_from_array(batch_size, self.eng_lang_words)\n",
    "#     indic_lang_batch = self.get_batch_from_array(batch_size, self.indic_lang_words)\n",
    "#     self.shuffle_start_index += batch_size + 1\n",
    "\n",
    "#     # Reshuffle if 1 epoch is complete\n",
    "#     if self.shuffle_start_index >= len(self.eng_lang_words):\n",
    "#       random.shuffle(self.shuffle_indices)\n",
    "#       self.shuffle_start_index = 0\n",
    "\n",
    "#     return eng_lang_batch, indic_lang_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "NmKmTPyty7mI"
   },
   "outputs": [],
   "source": [
    "# def calc_accuracy(net, device = 'cpu', data = val_data):\n",
    "#     # net = net.eval().to(device)\n",
    "#     # predictions = []\n",
    "#     accuracy = 0\n",
    "#     count = 0\n",
    "#     for i in range(len(data)):\n",
    "#         eng_word, indic_word = [j for j in data[i].__dict__.values()]\n",
    "#         # gt = gt_rep(indic_word, indic_alpha2idx, device)\n",
    "\n",
    "#         # outputs = infer(net, eng_word, gt.shape[0], device)\n",
    "#         output = translit_infer(net, eng_word, eng, indic, device, max_length=50)\n",
    "#         correct = 0\n",
    "\n",
    "#         for index, char in output:\n",
    "#           if char == indic_word[index]:\n",
    "#             correct += 1\n",
    "\n",
    "\n",
    "#         char_level_acc = correct/len(indic_word)\n",
    "        \n",
    "#         if char_level_acc == 1.0:\n",
    "#           count += 1\n",
    "#     print(count)\n",
    "#     accuracy = count/len(data)\n",
    "    \n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "8qyO9m_DcJAW"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  # input_size = size of vocab\n",
    "  # embedding_size - to map each input to some d dim space\n",
    "  # num_layers \n",
    "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.dropout = nn.Dropout(p)\n",
    "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "\n",
    "\n",
    "  # x - vector of indices, each token of sentence will be mapped to an index in vocab\n",
    "  def forward(self, x):\n",
    "    # x shape: (seq_length, N) -> N: batch size\n",
    "\n",
    "    embedding = self.dropout(self.embedding(x))\n",
    "    #embedding shape: (seq_length, N, embedding_size) -> each word(seq_length) will be mapped to an embedding of embedding_size\n",
    "\n",
    "    outputs, (hidden, cell) = self.rnn(embedding)\n",
    "    return hidden, cell\n",
    "    #outputs not important, only hidden and cell is important as they form the context vector\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  # input_size - size of english vocab, output_size same as input_size\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "      super(Decoder,self).__init__()\n",
    "      self.hidden_size = hidden_size\n",
    "      self.num_layers = num_layers\n",
    "\n",
    "      self.dropout = nn.Dropout(p)\n",
    "      self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "      self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "\n",
    "      # Note: Hidden size of encoder and decoder must be the same\n",
    "\n",
    "      self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "      #Prediction is done one word at a time, but for N words in a batch, so (1,N)\n",
    "      # shape of x: (N) but we want (1, N) -> i.e. N batches of a single word, Decoder predicts 1 word at a time, taking prev Decoder output and prev hidden cell.\n",
    "      x = x.unsqueeze(0)\n",
    "\n",
    "      embedding = self.dropout(self.embedding(x))\n",
    "      # embedding shape: (1,N, embedding_size)\n",
    "\n",
    "      # all 3 important now: outputs, hidden, cell\n",
    "      outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "      # shape of outputs: (1, N, hidden_size)\n",
    "\n",
    "      predictions = self.fc(outputs)\n",
    "      # shape of predictions: (1, N, length_of_vocab) -> (N, length_of_vocab)\n",
    "\n",
    "      predictions = predictions.squeeze(0)\n",
    "\n",
    "      return predictions, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super(Seq2Seq, self).__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def forward(self, source, target, teacher_force_ratio = 0.5):\n",
    "    batch_size = source.shape[1] # source dim: (target_len, N) -> N: batch size\n",
    "    target_len = target.shape[0]\n",
    "    target_vocab_size = len(indic.vocab)\n",
    "\n",
    "    # predict 1 word at a time, but do it for an entire batch, every vector will be of that entire vocab size\n",
    "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "    hidden, cell = self.encoder(source)\n",
    "\n",
    "    # Grab start token\n",
    "    x = target[0]\n",
    "\n",
    "    # send to decoder word by word\n",
    "    for t in range(1, target_len):\n",
    "      output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "      outputs[t] = output # adding along 1st dimension -> target_len\n",
    "      # output dim -> (N, english_vocab_size) -> doing argmax along this dimension, we'll get index corresponding to best guess that decoder outputted.\n",
    "      best_guess = output.argmax(1)\n",
    "\n",
    "      # implementing ground truth\n",
    "      x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "NJBBW4u3zwae"
   },
   "outputs": [],
   "source": [
    "\n",
    "def translit_infer(model, word, eng, indic, device, max_length=50):\n",
    "    tokens = tokenize_eng(word)\n",
    "\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, eng.init_token)\n",
    "    tokens.append(eng.eos_token)\n",
    "\n",
    "\n",
    "    text_to_indices = [eng.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    word_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(word_tensor)\n",
    "\n",
    "    outputs = [indic.vocab.stoi[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_char = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(previous_char, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == indic.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translit_res = [indic.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    # remove start token\n",
    "    translit_res_word = ''\n",
    "    translit_res = translit_res[1:]\n",
    "    # return translit_res\n",
    "    for i in translit_res:\n",
    "      if i != \"<eos>\":\n",
    "        translit_res_word += indic_idx2alpha[i]\n",
    "      else:\n",
    "        break\n",
    "    return translit_res_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "8yUgp84Mz3t_"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=f\"{indic_lang}_2_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "EHVn3O5_ITh8"
   },
   "outputs": [],
   "source": [
    "from IPython.utils.path import target_outdated\n",
    "def check_accuracy(loader, model, input_shape=None, toggle_eval=True, print_accuracy=True):\n",
    "    if toggle_eval:\n",
    "        model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loader.create_batches()\n",
    "        for batch in loader.batches:\n",
    "          for example in batch:\n",
    "            num_samples += 1\n",
    "            eng_word = \"\".join([eng_idx2alpha[val] for val in example.eng])\n",
    "            indic_word = \"\".join([indic_idx2alpha[val2] for val2 in example.indic])\n",
    "            indic_pred = translit_infer(model, eng_word, eng, indic, device, max_length=50)\n",
    "            \n",
    "            if indic_pred == indic_word:\n",
    "              num_correct += 1\n",
    "\n",
    "    accuracy = num_correct / num_samples\n",
    "    if toggle_eval:\n",
    "        model.train()\n",
    "#     if print_accuracy:\n",
    "#         print(f\"Accuracy on validation set: {accuracy * 100:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FqtOKyDGm7Oh"
   },
   "outputs": [],
   "source": [
    "### Now model is ready to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69 µs, sys: 0 ns, total: 69 µs\n",
      "Wall time: 76.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training Hyperparameters\n",
    "num_epochs =5\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "# Model Hyperparameters\n",
    "load_model = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size_encoder = len(eng.vocab)\n",
    "input_size_decoder = len(indic.vocab)\n",
    "output_size = len(indic.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 512\n",
    "num_layers = 8\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rggL-b4Mm-4w",
    "outputId": "5a7efc42-11f3-439c-acce-d46325f33dee"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    train_iterator, val_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data, test_data),\n",
    "    batch_size = batch_size,\n",
    "    # Examples of similar length will be in same batch to minimize padding and save on compute\n",
    "    sort_within_batch = True,\n",
    "    sort_key = lambda x: len(x.eng),\n",
    "    device = device)\n",
    "\n",
    "\n",
    "\n",
    "    encoder_net = Encoder(\n",
    "        input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
    "        ).to(device)\n",
    "    decoder_net = Decoder(\n",
    "        input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout\n",
    "        ).to(device)\n",
    "\n",
    "    model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    pad_idx = indic.vocab.stoi['<pad>']\n",
    "    # if all examples in batch are of similar length, don't incur penalty for this padding\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "    if load_model:\n",
    "      load_checkpoint(torch.load(f'{indic_lang}_checkpoint.pth.tar'), model, optimizer)\n",
    "\n",
    "    word = 'bachta'\n",
    "    og_translit = 'बचता'\n",
    "    acc_val_prev = 0\n",
    "    acc_val_current = 0\n",
    "    for epoch in range(num_epochs):\n",
    "      print(f'Epoch [{epoch+1} / {num_epochs}]')\n",
    "\n",
    "      checkpoint = {\n",
    "          'state_dict': model.state_dict(),\n",
    "          'optimizer': optimizer.state_dict()\n",
    "      }\n",
    "      if acc_val_current > acc_val_prev:\n",
    "\n",
    "          if os.path.exists(f'{indic_lang}_checkpoint_new_{acc_val_prev*100:.2f}.pth.tar'):\n",
    "              os.remove(f'{indic_lang}_checkpoint_new_{acc_val_prev*100:.2f}.pth.tar')\n",
    "          acc_val_prev = acc_val_current\n",
    "          save_checkpoint(checkpoint, f'{indic_lang}_checkpoint_new_{acc_val_current*100:.2f}.pth.tar')\n",
    "\n",
    "\n",
    "\n",
    "      loop = tqdm(enumerate(train_iterator), total=len(train_iterator))\n",
    "      for batch_idx, batch in loop:\n",
    "        inp_data = batch.eng.to(device)\n",
    "        target = batch.indic.to(device)\n",
    "\n",
    "        output = model(inp_data, target)\n",
    "        # output shape: (target_len, batch_size, output_dim)\n",
    "\n",
    "        #basically reshape output keeping last output_dim same\n",
    "        output = output[1:].reshape(-1, output.shape[2]) # so that first start token is not sent to out model\n",
    "        # target -> (target_len, batch_size)\n",
    "        target = target[1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # to avoid exploding gradients, clip them when they are above a threshold\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "      model.eval() # turns off Dropout\n",
    "      translit_res = translit_infer(model, word, eng, indic, device, max_length=50)\n",
    "      print(f'Translated example word:  English: {word}, Actual: {og_translit}, Predicted: {translit_res}')\n",
    "      model.train()\n",
    "\n",
    "      print('Computing Loss and Validation Accuracy...')\n",
    "      acc_val_current = check_accuracy(val_iterator, model, input_shape=None, toggle_eval=True, print_accuracy=True)\n",
    "      print(f'Training Loss: {loss.item()}, Validation Accuracy: {acc_val_current * 100:.2f}%')\n",
    "      print('--------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 400/400 [00:52<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated example word:  English: bachta, Actual: बचता, Predicted: जााा\n",
      "Computing Loss and Validation Accuracy...\n",
      "Training Loss: 2.953324317932129, Validation Accuracy: 0.00%\n",
      "--------------------------\n",
      "Epoch [2 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 400/400 [00:55<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated example word:  English: bachta, Actual: बचता, Predicted: बा्ा\n",
      "Computing Loss and Validation Accuracy...\n",
      "Training Loss: 2.57366943359375, Validation Accuracy: 0.10%\n",
      "--------------------------\n",
      "Epoch [3 / 5]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 400/400 [00:56<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated example word:  English: bachta, Actual: बचता, Predicted: बारत\n",
      "Computing Loss and Validation Accuracy...\n",
      "Training Loss: 2.2431225776672363, Validation Accuracy: 0.68%\n",
      "--------------------------\n",
      "Epoch [4 / 5]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 400/400 [00:56<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated example word:  English: bachta, Actual: बचता, Predicted: बा्ता\n",
      "Computing Loss and Validation Accuracy...\n",
      "Training Loss: 1.8380482196807861, Validation Accuracy: 2.42%\n",
      "--------------------------\n",
      "Epoch [5 / 5]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 400/400 [00:56<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated example word:  English: bachta, Actual: बचता, Predicted: बच्ाा\n",
      "Computing Loss and Validation Accuracy...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
