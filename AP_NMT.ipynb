{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcwinMv159j3IV28Rs62ve",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oikn2018/CS6910_assignment_3/blob/main/AP_NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0\n",
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "! pip install wget\n",
        "! pip install gdown\n",
        "! pip install --upgrade gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KreNpqQWnde",
        "outputId": "06801c0e-708c-4731-c724-fb88b6aefb55"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.27.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.65.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wYmZkdTHV1gb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import gdown\n",
        "# import wandb\n",
        "from io import open\n",
        "import string, time, math\n",
        "import wget\n",
        "from zipfile import ZipFile\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "# from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "# import numpy as np\n",
        "import spacy\n",
        "# import random\n",
        "# from torch.utils.tensorboard import SummaryWriter # to print to tensorboard\n",
        "# from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "# CUDA\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "# torch.backends.cudnn.deterministic=True\n",
        "# torch.backends.cudnn.benchmark=False"
      ],
      "metadata": {
        "id": "gaw2m7L5Dj0j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the Dataset\n",
        "url = 'https://drive.google.com/uc?id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw&export=download'\n",
        "# filename = os.path.basename(url)\n",
        "# print(filename)\n",
        "\n",
        "if not os.path.exists(\"aksharantar_sampled\"):\n",
        "  filename = gdown.download(url = url, quiet=False, fuzzy=True)\n",
        "  print(filename)\n",
        "  with ZipFile(filename, 'r') as z:\n",
        "    print('Extracting files...')\n",
        "    z.extractall()\n",
        "    print('Done!')\n",
        "  os.remove(filename)"
      ],
      "metadata": {
        "id": "q87jZlq8D--z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_alpha = 'abcdefghijklmnopqrstuvwxyz'\n",
        "pad_char = '<PAD>'\n",
        "\n",
        "eng_alpha2idx = {pad_char: 0}\n",
        "for index, alpha in enumerate(eng_alpha):\n",
        "  eng_alpha2idx[alpha] = index+1\n",
        "\n",
        "print(eng_alpha2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1cP5vkMgOD3",
        "outputId": "b57b6328-3e95-4dd0-9c9c-f46b35bc69ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<PAD>': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bengali Unicode Hex Range: 2432-2558\n",
        "# Hindi Unicode Hex Range: 2304-2431\n",
        "\n",
        "min_range = 2304\n",
        "max_range = 2431\n",
        "\n",
        "# if indic_lang == 'ben':\n",
        "#   min_range = 2432\n",
        "#   max_range = 2558\n",
        "# elif indic_lang == 'hindi':\n",
        "#   min_range = 2304\n",
        "#   max_range = 2431\n",
        "\n",
        "indic_alpha = [chr(alpha) for alpha in range(min_range, max_range + 1)]\n",
        "print(indic_alpha)\n",
        "indic_alpha_size = len(indic_alpha)\n",
        "\n",
        "indic_alpha2idx = {pad_char: 0}\n",
        "for index, alpha in enumerate(indic_alpha):\n",
        "  indic_alpha2idx[alpha] = index+1\n",
        "\n",
        "print(indic_alpha2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beH3ItF4gTTd",
        "outputId": "72fe6c23-c7e3-4dd5-8504-0605cfcfb771"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ऀ', 'ँ', 'ं', 'ः', 'ऄ', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऌ', 'ऍ', 'ऎ', 'ए', 'ऐ', 'ऑ', 'ऒ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'ऩ', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल', 'ळ', 'ऴ', 'व', 'श', 'ष', 'स', 'ह', 'ऺ', 'ऻ', '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'ॅ', 'ॆ', 'े', 'ै', 'ॉ', 'ॊ', 'ो', 'ौ', '्', 'ॎ', 'ॏ', 'ॐ', '॑', '॒', '॓', '॔', 'ॕ', 'ॖ', 'ॗ', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', 'य़', 'ॠ', 'ॡ', 'ॢ', 'ॣ', '।', '॥', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', '॰', 'ॱ', 'ॲ', 'ॳ', 'ॴ', 'ॵ', 'ॶ', 'ॷ', 'ॸ', 'ॹ', 'ॺ', 'ॻ', 'ॼ', 'ॽ', 'ॾ', 'ॿ']\n",
            "{'<PAD>': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indic_idx2alpha = {v: k for k, v in indic_alpha2idx.items()}\n",
        "eng_idx2alpha = {v: k for k, v in eng_alpha2idx.items()}"
      ],
      "metadata": {
        "id": "tNrcqaQQ6j-5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_indic(string):\n",
        "  # return string.split()\n",
        "  char_list =  [*string]\n",
        "  char_list = [indic_alpha2idx[char] for char in char_list]\n",
        "  return char_list\n",
        "\n",
        "def tokenize_eng(string):\n",
        "  # return string.split()\n",
        "  char_list =  [*string]\n",
        "  char_list = [eng_alpha2idx[char] for char in char_list]\n",
        "  return char_list"
      ],
      "metadata": {
        "id": "3j6vZGlyEuKo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenize_indic('बिन्द्या'))\n",
        "print(tokenize_eng('hello'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYEkyS3KLRh1",
        "outputId": "771a478e-5377-4f1b-d003-0db86a4e7522"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[45, 64, 41, 78, 39, 78, 48, 63]\n",
            "[8, 5, 12, 12, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change Indic Language here\n",
        "# indic_lang = 'ben'\n",
        "indic_lang = 'hin'"
      ],
      "metadata": {
        "id": "0PulDBIPFWhj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing python package\n",
        "import pandas as pd\n",
        "  \n",
        "file_names = ['test', 'train', 'valid']\n",
        "\n",
        "for index, file_name in enumerate(file_names):\n",
        "  # read contents of csv file\n",
        "  file = pd.read_csv(f'aksharantar_sampled/{indic_lang}/{indic_lang}_{file_name}.csv')\n",
        "  # print(\"\\nOriginal file:\")\n",
        "  # print(file)\n",
        "    \n",
        "  # adding header\n",
        "  headerList = ['eng', f'{indic_lang}']\n",
        "    \n",
        "  # converting data frame to csv\n",
        "  file.to_csv(f'aksharantar_sampled/{indic_lang}/{indic_lang}_{file_name}.csv', header=headerList, index=False)\n",
        "    \n",
        "  # display modified csv file\n",
        "  # file2 = pd.read_csv(f'aksharantar_sampled/{indic_lang}/{indic_lang}_valid_2.csv')\n",
        "  # print('\\nModified file:')\n",
        "  # print(file2)"
      ],
      "metadata": {
        "id": "FCuwdH7VHXE1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng = Field(sequential=True, use_vocab=True, tokenize=tokenize_eng, init_token='<sos>', eos_token='<eos>')\n",
        "indic = Field(sequential=True, use_vocab=True, tokenize=tokenize_indic, init_token='<sos>', eos_token='<eos>')"
      ],
      "metadata": {
        "id": "gUYK7Us1EDqD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tig4Kqp0MUsd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fields={'eng': ('eng', eng), f'{indic_lang}': ('indic', indic)}\n",
        "\n",
        "path_name = f'aksharantar_sampled/{indic_lang}'\n",
        "train_name = f'{indic_lang}_train.csv'\n",
        "val_name = f'{indic_lang}_valid.csv'\n",
        "test_name = f'{indic_lang}_test.csv'\n",
        "train_data, val_data, test_data = TabularDataset.splits(\n",
        "    path= path_name,\n",
        "    train=train_name,\n",
        "    validation=val_name,\n",
        "    test=test_name,\n",
        "    format='csv',\n",
        "    fields=fields\n",
        ")"
      ],
      "metadata": {
        "id": "5RFKlw5MEx6l"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0].__dict__.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwT-AtD0MaKK",
        "outputId": "16e7ac88-caa4-44af-d547-800e191afbf7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['eng', 'indic'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0].__dict__.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIaEB78IKrdL",
        "outputId": "5384ea01-792f-4175-a090-c03a4aad461d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_values([[2, 9, 14, 4, 8, 25, 1], [45, 64, 41, 78, 39, 78, 48, 63]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_w, indic_w = [i for i in train_data[0].__dict__.values()]\n",
        "print(eng_w)\n",
        "for val in eng_w:\n",
        "  print(eng_idx2alpha[val])"
      ],
      "metadata": {
        "id": "iOx6Q3ZeAx6p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d7049cd-e605-4986-991c-7470cc0f65f9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 9, 14, 4, 8, 25, 1]\n",
            "b\n",
            "i\n",
            "n\n",
            "d\n",
            "h\n",
            "y\n",
            "a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng.build_vocab(train_data, max_size = 1000, min_freq = 1)\n",
        "indic.build_vocab(train_data, max_size = 1000, min_freq = 1)"
      ],
      "metadata": {
        "id": "_9owJfZJMFJq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(eng.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCoDONfrVmPY",
        "outputId": "90a03353-a104-40d2-8a13-863195f84bb5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(eng.vocab.__dict__.keys())\n",
        "# print(eng.vocab.help?)\n",
        "print(help(eng.vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-IuQ5nHu2Ma",
        "outputId": "499f51ac-e47c-430e-b5e0-9a9569cc7153"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['freqs', 'itos', 'unk_index', 'stoi', 'vectors'])\n",
            "Help on Vocab in module torchtext.vocab object:\n",
            "\n",
            "class Vocab(builtins.object)\n",
            " |  Vocab(counter, max_size=None, min_freq=1, specials=['<unk>', '<pad>'], vectors=None, unk_init=None, vectors_cache=None, specials_first=True)\n",
            " |  \n",
            " |  Defines a vocabulary object that will be used to numericalize a field.\n",
            " |  \n",
            " |  Attributes:\n",
            " |      freqs: A collections.Counter object holding the frequencies of tokens\n",
            " |          in the data used to build the Vocab.\n",
            " |      stoi: A collections.defaultdict instance mapping token strings to\n",
            " |          numerical identifiers.\n",
            " |      itos: A list of token strings indexed by their numerical identifiers.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __eq__(self, other)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __getitem__(self, token)\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __init__(self, counter, max_size=None, min_freq=1, specials=['<unk>', '<pad>'], vectors=None, unk_init=None, vectors_cache=None, specials_first=True)\n",
            " |      Create a Vocab object from a collections.Counter.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          counter: collections.Counter object holding the frequencies of\n",
            " |              each value found in the data.\n",
            " |          max_size: The maximum size of the vocabulary, or None for no\n",
            " |              maximum. Default: None.\n",
            " |          min_freq: The minimum frequency needed to include a token in the\n",
            " |              vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
            " |          specials: The list of special tokens (e.g., padding or eos) that\n",
            " |              will be prepended to the vocabulary. Default: ['<unk'>, '<pad>']\n",
            " |          vectors: One of either the available pretrained vectors\n",
            " |              or custom pretrained vectors (see Vocab.load_vectors);\n",
            " |              or a list of aforementioned vectors\n",
            " |          unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
            " |              to zero vectors; can be any function that takes in a Tensor and\n",
            " |              returns a Tensor of the same size. Default: torch.Tensor.zero_\n",
            " |          vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
            " |          specials_first: Whether to add special tokens into the vocabulary at first.\n",
            " |              If it is False, they are added into the vocabulary at last.\n",
            " |              Default: True.\n",
            " |  \n",
            " |  __len__(self)\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  extend(self, v, sort=False)\n",
            " |  \n",
            " |  load_vectors(self, vectors, **kwargs)\n",
            " |      Arguments:\n",
            " |          vectors: one of or a list containing instantiations of the\n",
            " |              GloVe, CharNGram, or Vectors classes. Alternatively, one\n",
            " |              of or a list of available pretrained vectors:\n",
            " |              charngram.100d\n",
            " |              fasttext.en.300d\n",
            " |              fasttext.simple.300d\n",
            " |              glove.42B.300d\n",
            " |              glove.840B.300d\n",
            " |              glove.twitter.27B.25d\n",
            " |              glove.twitter.27B.50d\n",
            " |              glove.twitter.27B.100d\n",
            " |              glove.twitter.27B.200d\n",
            " |              glove.6B.50d\n",
            " |              glove.6B.100d\n",
            " |              glove.6B.200d\n",
            " |              glove.6B.300d\n",
            " |          Remaining keyword arguments: Passed to the constructor of Vectors classes.\n",
            " |  \n",
            " |  set_vectors(self, stoi, vectors, dim, unk_init=<method 'zero_' of 'torch._C._TensorBase' objects>)\n",
            " |      Set the vectors for the Vocab instance from a collection of Tensors.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          stoi: A dictionary of string to the index of the associated vector\n",
            " |              in the `vectors` input argument.\n",
            " |          vectors: An indexed iterable (or other structure supporting __getitem__) that\n",
            " |              given an input index, returns a FloatTensor representing the vector\n",
            " |              for the token associated with the index. For example,\n",
            " |              vector[stoi[\"string\"]] should return the vector for \"string\".\n",
            " |          dim: The dimensionality of the vectors.\n",
            " |          unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
            " |              to zero vectors; can be any function that takes in a Tensor and\n",
            " |              returns a Tensor of the same size. Default: torch.Tensor.zero_\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  UNK = '<unk>'\n",
            " |  \n",
            " |  __hash__ = None\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(indic.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDioMDH6uwsD",
        "outputId": "27379fcc-a8b1-4a5e-aeb7-e509859c8ea0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_iter, val_iter, test_iter = BucketIterator.splits(\n",
        "#     (train_data, val_data, test_data), \n",
        "#     batch_size = 2, \n",
        "#     sort_within_batch=True,  \n",
        "#     sort_key = lambda x: len(x.eng),\n",
        "#     shuffle=True,\n",
        "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
      ],
      "metadata": {
        "id": "d1eIGdzqM_19"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(train_iter)\n",
        "# train_iter.create_batches()"
      ],
      "metadata": {
        "id": "WYavrckKOFnP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch in train_iter:\n",
        "#   print(batch)\n",
        "  # for example in batch:\n",
        "  #   print(type(example.eng))\n",
        "\n",
        "  # break"
      ],
      "metadata": {
        "id": "nZ0JmGBrS5w2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python3 -m spacy download de_core_news_md\n",
        "# !python3 -m spacy download en_core_web_md \n"
      ],
      "metadata": {
        "id": "vX48JEqWa9hT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy_ger = spacy.load('de_core_news_md')\n",
        "# spacy_eng = spacy.load('en_core_web_md')\n",
        "\n",
        "# # Takes some text and returns a list of strings\n",
        "# def tokenizer_ger(text):\n",
        "#   return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
        "\n",
        "# def tokenizer_eng(text):\n",
        "#   return [tok.text for tok in spacy_eng.tokenizer(text)]"
      ],
      "metadata": {
        "id": "qH7x5hi7WLbt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(spacy_ger)"
      ],
      "metadata": {
        "id": "dxflZSfc4404"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# string = 'Hello my name is Oikantik'\n",
        "# print(tokenizer_eng(string))"
      ],
      "metadata": {
        "id": "_pO0fT8luWIx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# german = Field(tokenize = tokenizer_ger, lower=True, init_token='<sos>', eos_token='<eos>')\n",
        "# english = Field(tokenize = tokenizer_eng, lower=True, init_token='<sos>', eos_token='<eos>')"
      ],
      "metadata": {
        "id": "ro41iDr3aGOj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(english)"
      ],
      "metadata": {
        "id": "Xoef1Hz-uMWC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data, val_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(german, english))"
      ],
      "metadata": {
        "id": "yt4UH821bmoq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_data[:3])"
      ],
      "metadata": {
        "id": "KKIhfQ0-t1ZI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # if a word is present at least 2 times, then only we'll add to our vocab\n",
        "# german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "# english.build_vocab(train_data, max_size=10000, min_freq=2)"
      ],
      "metadata": {
        "id": "0lsXCA5mb3v-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eng_alpha = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "# pad_char = '<PAD>'\n",
        "\n",
        "# eng_alpha2idx = {pad_char: 0}\n",
        "# for index, alpha in enumerate(eng_alpha):\n",
        "#   eng_alpha2idx[alpha] = index+1\n",
        "\n",
        "# print(eng_alpha2idx)"
      ],
      "metadata": {
        "id": "GQ-J3aIm7SPK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Bengali Unicode Hex Range: 2432-2558\n",
        "# # Hindi Unicode Hex Range: 2304-2431\n",
        "\n",
        "# min_range = 2304\n",
        "# max_range = 2431\n",
        "\n",
        "# if indic_lang == 'ben':\n",
        "#   min_range = 2432\n",
        "#   max_range = 2558\n",
        "# elif indic_lang == 'hin':\n",
        "#   min_range = 2304\n",
        "#   max_range = 2431\n",
        "\n",
        "# indic_alpha = [chr(alpha) for alpha in range(min_range, max_range + 1)]\n",
        "# print(indic_alpha)\n",
        "# indic_alpha_size = len(indic_alpha)\n",
        "\n",
        "# indic_alpha2idx = {pad_char: 0}\n",
        "# for index, alpha in enumerate(indic_alpha):\n",
        "#   indic_alpha2idx[alpha] = index+1\n",
        "\n",
        "# print(indic_alpha2idx)"
      ],
      "metadata": {
        "id": "zQAlqawP7PJ_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# indic_langs = sorted([indic_lang for indic_lang in os.listdir(\"aksharantar_sampled\") if indic_lang != '.DS_Store'])\n",
        "# print(indic_langs)"
      ],
      "metadata": {
        "id": "9p1Gf44P7EWQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class TransLit_DataLoader(Dataset):\n",
        "#   def __init__(self, filename):\n",
        "#     self.eng_lang_words, self.indic_lang_words = self.readDataset(filename)\n",
        "#     self.shuffle_indices = list(range(len(self.eng_lang_words)))\n",
        "#     random.shuffle(self.shuffle_indices)\n",
        "#     self.shuffle_start_index = 0\n",
        "\n",
        "#   def __len__(self):\n",
        "#     return len(self.eng_lang_words)\n",
        "\n",
        "#   def __getitem__(self, idx):\n",
        "#     return self.eng_lang_words[idx], self.indic_lang_words[idx]\n",
        "\n",
        "#   def readDataset(self, filename):\n",
        "#     X = []\n",
        "#     y = []\n",
        "#     # data = []\n",
        "\n",
        "#     with open(filename, 'r') as f:\n",
        "#       for line in f:\n",
        "#         line = line.split(',')\n",
        "#         eng_word = line[0].strip()\n",
        "#         indic_word = line[1].strip()\n",
        "#         X.append(eng_word)\n",
        "#         y.append(indic_word)\n",
        "#         # data_train.append((eng_word, indic_word))\n",
        "#     return X, y\n",
        "\n",
        "#   def get_random_sample(self):\n",
        "#     return self.__getitem__(np.random.randint(len(self.eng_lang_words)))\n",
        "\n",
        "#   def get_batch_from_array(self, batch_size, array):\n",
        "#     end = self.shuffle_start_index + batch_size\n",
        "#     batch = []\n",
        "#     if end >= len(self.eng_lang_words):\n",
        "#       batch = [array[i] for i in self.shuffle_indices[0:end%len(self.eng_lang_words)]]\n",
        "#     return batch + [array[i] for i in self.shuffle_indices[self.shuffle_start_index:end]]\n",
        "\n",
        "#   def get_batch(self, batch_size, postprocess = True):\n",
        "#     eng_lang_batch = self.get_batch_from_array(batch_size, self.eng_lang_words)\n",
        "#     indic_lang_batch = self.get_batch_from_array(batch_size, self.indic_lang_words)\n",
        "#     self.shuffle_start_index += batch_size + 1\n",
        "\n",
        "#     # Reshuffle if 1 epoch is complete\n",
        "#     if self.shuffle_start_index >= len(self.eng_lang_words):\n",
        "#       random.shuffle(self.shuffle_indices)\n",
        "#       self.shuffle_start_index = 0\n",
        "\n",
        "#     return eng_lang_batch, indic_lang_batch"
      ],
      "metadata": {
        "id": "dGRPlmpl7ELU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_train = TransLit_DataLoader(f'aksharantar_sampled/{indic_lang}/{indic_lang}_train.csv')\n",
        "# data_val = TransLit_DataLoader(f'aksharantar_sampled/{indic_lang}/{indic_lang}_valid.csv')\n",
        "# data_test = TransLit_DataLoader(f'aksharantar_sampled/{indic_lang}/{indic_lang}_test.csv')"
      ],
      "metadata": {
        "id": "kn19K4vE60pJ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(net, device = 'cpu', data = val_data):\n",
        "    # net = net.eval().to(device)\n",
        "    # predictions = []\n",
        "    accuracy = 0\n",
        "    count = 0\n",
        "    for i in range(len(data)):\n",
        "        eng_word, indic_word = [j for j in data[i].__dict__.values()]\n",
        "        # gt = gt_rep(indic_word, indic_alpha2idx, device)\n",
        "\n",
        "        # outputs = infer(net, eng_word, gt.shape[0], device)\n",
        "        output = translit_infer(net, eng_word, eng, indic, device, max_length=50)\n",
        "        correct = 0\n",
        "        # indic_output = ''\n",
        "\n",
        "\n",
        "        # for index, out in enumerate(outputs):\n",
        "        #         # val, indices = out.topk(1)\n",
        "                \n",
        "        #         indic_pos = indices.tolist()[0]\n",
        "        #         # print(indic_pos)\n",
        "                \n",
        "        #         if indic_pos[0] == gt[index][0]:\n",
        "        #           correct += 1\n",
        "        #           if indic_pos[0] != 0:\n",
        "        #             indic_output += indic_alpha[indic_pos[0]-1]\n",
        "\n",
        "        for index, char in output:\n",
        "          if char == indic_word[index]:\n",
        "            correct += 1\n",
        "\n",
        "\n",
        "        char_level_acc = correct/len(indic_word)\n",
        "\n",
        "        # if index ==2:\n",
        "        #   break\n",
        "        \n",
        "        if char_level_acc == 1.0:\n",
        "          count += 1\n",
        "#           print(f'{eng_word}-{indic_word}-{indic_output}')\n",
        "        # if char_level_acc > 0.65 and char_level_acc != 1.0:\n",
        "        #     print(f'{eng_word}-{indic_word}-{indic_output}')\n",
        "    print(count)\n",
        "    accuracy = count/len(data)\n",
        "    \n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "NmKmTPyty7mI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  # input_size = size of vocab\n",
        "  # embedding_size - to map each input to some d dim space\n",
        "  # num_layers \n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "\n",
        "  # x - vector of indices, each token of sentence will be mapped to an index in vocab\n",
        "  def forward(self, x):\n",
        "    # x shape: (seq_length, N) -> N: batch size\n",
        "\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    #embedding shape: (seq_length, N, embedding_size) -> each word(seq_length) will be mapped to an embedding of embedding_size\n",
        "\n",
        "    outputs, (hidden, cell) = self.rnn(embedding)\n",
        "    return hidden, cell\n",
        "    #outputs not important, only hidden and cell is important as they form the context vector\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  # input_size - size of english vocab, output_size same as input_size\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "      super(Decoder,self).__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.num_layers = num_layers\n",
        "\n",
        "      self.dropout = nn.Dropout(p)\n",
        "      self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "      self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "      # Note: Hidden size of encoder and decoder must be the same\n",
        "\n",
        "      self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "      #Prediction is done one word at a time, but for N words in a batch, so (1,N)\n",
        "      # shape of x: (N) but we want (1, N) -> i.e. N batches of a single word, Decoder predicts 1 word at a time, taking prev Decoder output and prev hidden cell.\n",
        "      x = x.unsqueeze(0)\n",
        "\n",
        "      embedding = self.dropout(self.embedding(x))\n",
        "      # embedding shape: (1,N, embedding_size)\n",
        "\n",
        "      # all 3 important now: outputs, hidden, cell\n",
        "      outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "      # shape of outputs: (1, N, hidden_size)\n",
        "\n",
        "      predictions = self.fc(outputs)\n",
        "      # shape of predictions: (1, N, length_of_vocab) -> (N, length_of_vocab)\n",
        "\n",
        "      predictions = predictions.squeeze(0)\n",
        "\n",
        "      return predictions, hidden, cell\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, source, target, teacher_force_ratio = 0.5):\n",
        "    batch_size = source.shape[1] # source dim: (target_len, N) -> N: batch size\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(indic.vocab)\n",
        "\n",
        "    # predict 1 word at a time, but do it for an entire batch, every vector will be of that entire vocab size\n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "    hidden, cell = self.encoder(source)\n",
        "\n",
        "    # Grab start token\n",
        "    x = target[0]\n",
        "\n",
        "    # send to decoder word by word\n",
        "    for t in range(1, target_len):\n",
        "      output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "\n",
        "      outputs[t] = output # adding along 1st dimension -> target_len\n",
        "      # output dim -> (N, english_vocab_size) -> doing argmax along this dimension, we'll get index corresponding to best guess that decoder outputted.\n",
        "      best_guess = output.argmax(1)\n",
        "\n",
        "      # implementing ground truth\n",
        "      x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "    return outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "8qyO9m_DcJAW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def translit_infer(model, word, eng, indic, device, max_length=50):\n",
        "    tokens = tokenize_eng(word)\n",
        "\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\n",
        "    tokens.insert(0, eng.init_token)\n",
        "    tokens.append(eng.eos_token)\n",
        "\n",
        "\n",
        "    text_to_indices = [eng.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    # Convert to Tensor\n",
        "    word_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(word_tensor)\n",
        "\n",
        "    outputs = [indic.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_char = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_char, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == indic.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translit_res = [indic.vocab.itos[idx] for idx in outputs]\n",
        "\n",
        "    # remove start token\n",
        "    translit_res_word = ''\n",
        "    translit_res = translit_res[1:]\n",
        "    # return translit_res\n",
        "    for i in translit_res:\n",
        "      if i != \"<eos>\":\n",
        "        translit_res_word += indic_idx2alpha[i]\n",
        "      else:\n",
        "        break\n",
        "    return translit_res_word\n"
      ],
      "metadata": {
        "id": "NJBBW4u3zwae"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, filename=f\"{indic_lang}_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ],
      "metadata": {
        "id": "8yUgp84Mz3t_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.utils.path import target_outdated\n",
        "def check_accuracy(loader, model, input_shape=None, toggle_eval=True, print_accuracy=True):\n",
        "    if toggle_eval:\n",
        "        model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loader.create_batches()\n",
        "        for batch in loader.batches:\n",
        "          for example in batch:\n",
        "            num_samples += 1\n",
        "            eng_word = \"\".join([eng_idx2alpha[val] for val in example.eng])\n",
        "            indic_word = \"\".join([indic_idx2alpha[val2] for val2 in example.indic])\n",
        "            indic_pred = translit_infer(model, eng_word, eng, indic, device, max_length=50)\n",
        "            \n",
        "            if indic_pred == indic_word:\n",
        "              num_correct += 1\n",
        "\n",
        "    accuracy = num_correct / num_samples\n",
        "    if toggle_eval:\n",
        "        model.train()\n",
        "    if print_accuracy:\n",
        "        print(f\"Accuracy on validation set: {accuracy * 100:.2f}%\")\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "EHVn3O5_ITh8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Now model is ready to train"
      ],
      "metadata": {
        "id": "FqtOKyDGm7Oh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Training Hyperparameters\n",
        "num_epochs =30\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "\n",
        "# Model Hyperparameters\n",
        "load_model = False\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "input_size_encoder = len(eng.vocab)\n",
        "input_size_decoder = len(indic.vocab)\n",
        "output_size = len(indic.vocab)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 3\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "\n",
        "train_iterator, val_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size = batch_size,\n",
        "    # Examples of similar length will be in same batch to minimize padding and save on compute\n",
        "    sort_within_batch = True,\n",
        "    sort_key = lambda x: len(x.eng),\n",
        "    device = device)\n",
        "\n",
        "\n",
        "\n",
        "encoder_net = Encoder(\n",
        "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
        "    ).to(device)\n",
        "decoder_net = Decoder(\n",
        "    input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout\n",
        "    ).to(device)\n",
        "\n",
        "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = indic.vocab.stoi['<pad>']\n",
        "# if all examples in batch are of similar length, don't incur penalty for this padding\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
        "\n",
        "if load_model:\n",
        "  load_checkpoint(torch.load(f'{indic_lang}_checkpoint.pth.tar'), model, optimizer)\n",
        "\n",
        "word = 'bachta'\n",
        "og_translit = 'बचता'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f'Epoch [{epoch+1} / {num_epochs}]')\n",
        "\n",
        "  checkpoint = {\n",
        "      'state_dict': model.state_dict(),\n",
        "      'optimizer': optimizer.state_dict()\n",
        "  }\n",
        "  save_checkpoint(checkpoint)\n",
        "\n",
        "  model.eval() # turns off Dropout\n",
        "  translit_res = translit_infer(model, word, eng, indic, device, max_length=50)\n",
        "  print(f'Translated example word:  English: {word}, Actual: {og_translit}, Predicted: {translit_res}')\n",
        "  model.train()\n",
        "\n",
        "\n",
        "  for batch_idx, batch in enumerate(train_iterator):\n",
        "    inp_data = batch.eng.to(device)\n",
        "    target = batch.indic.to(device)\n",
        "\n",
        "    output = model(inp_data, target)\n",
        "    # output shape: (target_len, batch_size, output_dim)\n",
        "\n",
        "    #basically reshape output keeping last output_dim same\n",
        "    output = output[1:].reshape(-1, output.shape[2]) # so that first start token is not sent to out model\n",
        "    # target -> (target_len, batch_size)\n",
        "    target = target[1:].reshape(-1)\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # to avoid exploding gradients, clip them when they are above a threshold\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "    optimizer.step()\n",
        "\n",
        "  print('Training Loss: ', loss.item())\n",
        "\n",
        "\n",
        "  check_accuracy(\n",
        "    val_iterator, model, input_shape=None, toggle_eval=True, print_accuracy=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rggL-b4Mm-4w",
        "outputId": "5a7efc42-11f3-439c-acce-d46325f33dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1 / 30]\n",
            "=> Saving checkpoint\n",
            "Translated example word:  English: bachta, Actual: बचता, Predicted: भबशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशशश\n",
            "Training Loss:  1.3955528736114502\n",
            "Accuracy on validation set: 19.34%\n",
            "Epoch [2 / 30]\n",
            "=> Saving checkpoint\n",
            "Translated example word:  English: bachta, Actual: बचता, Predicted: बच्ता\n",
            "Training Loss:  0.8256397247314453\n",
            "Accuracy on validation set: 30.60%\n",
            "Epoch [3 / 30]\n",
            "=> Saving checkpoint\n",
            "Translated example word:  English: bachta, Actual: बचता, Predicted: बच्ता\n",
            "Training Loss:  1.180772066116333\n",
            "Accuracy on validation set: 35.38%\n",
            "Epoch [4 / 30]\n",
            "=> Saving checkpoint\n",
            "Translated example word:  English: bachta, Actual: बचता, Predicted: बचता\n",
            "Training Loss:  0.44317397475242615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(indic.vocab.stoi['<eos>'])"
      ],
      "metadata": {
        "id": "czpOpuQZs4Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22026b02-42f7-43f6-88b9-2bafe5876a4d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "worprint(val_iterator.__dict__.keys())"
      ],
      "metadata": {
        "id": "cGwkS7zD8SI2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "af0712c3-3148-4a4c-f179-4a08002f3ec7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-95a1189628e1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mworprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'worprint' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, batch in enumerate(val_iterator):\n",
        "    inp_data = batch.eng\n",
        "    target = batch.indic\n",
        "    print(inp_data.shape, target.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "xwv-ssKrJnft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "42xgSCQ_J8-i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}